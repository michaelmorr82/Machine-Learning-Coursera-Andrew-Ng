{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # for data structuring and data analysis tools\n",
    "import numpy as np # arrays, algebra\n",
    "import matplotlib.pyplot as plt #2-d plotting\n",
    "import sympy\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d # 3 d plots\n",
    "\n",
    "import seaborn as sns # statistical data visualisation\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "\n",
    "from sympy.abc import theta\n",
    "\n",
    "plt.close(\"all\") # Clsoe all plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Up Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identy Maxtric of 5 x 5:\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "I_num = 5\n",
    "def warmUpExercise(I_num):\n",
    "    Ident = np.identity(I_num)\n",
    "    return(Ident)\n",
    "\n",
    "Ident = warmUpExercise(I_num)\n",
    "# print('Identity Matrix %d x %d'%(I_num, I_num))\n",
    "print('Identy Maxtric of %d x %d:'%(I_num, I_num))\n",
    "\n",
    "print(Ident)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex1data1.txt', delimiter = ',') #imports 96 x 2 array\n",
    "\n",
    "X = np.c_[np.ones(len(data)),data[:,0]] #concatinates column of 1's with first imported colums (x-data)\n",
    "\n",
    "y = np.c_[data[:,1]] # set y data to the second imported column\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,1], y, s = 30, c='r', marker = 'x', linewidths = 1)\n",
    "plt.xlim(4,24)\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s');\n",
    "plt.title('Raw Data Imported')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with one Variable (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: \n",
    "$ h_\\theta (x) = \\theta_0 + \\theta_1 x $\n",
    "\n",
    "Parameters/Features: \n",
    "$\\theta_0 $, $\\theta_1$\n",
    "\n",
    "m = number of training examples <br>\n",
    "n = number of features <br>\n",
    "i = interation of training examples <br>\n",
    "j = iterations of features<br>\n",
    "$\\alpha$ = learnign rate\n",
    "\n",
    "Cost Function (J): \n",
    "$J(\\theta_0, \\theta_1) = \\frac{1}{(2m)}\\sum_{i = 1}^m \\Bigl(h_\\theta (x^{(i)}) - y^{(i)}\\Bigr)^{2}$\n",
    "\n",
    "Goal:  minimise the Cost function  using parameters\n",
    "\n",
    "Algorithm: Gradient descent. Repeat Until Convergence (updating parameters simultaneously)\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J\\Bigl(\\theta_0, \\theta_1\\Bigr)$$\n",
    "where <br>\n",
    "for j = 0\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} J\\Bigl(\\theta_0, \\theta_1\\Bigr) = \\frac{1}{m}\\sum_{i = 1}^m \\Bigl(h_\\theta (x^{(i)}) - y^{(i)}\\Bigr)$$\n",
    "for j = 1,2,3...n\n",
    "$$\\frac{\\partial}{\\partial \\theta_1} J\\Bigl(\\theta_0, \\theta_1\\Bigr) = \\frac{1}{m}\\sum_{i = 1}^m \\Bigl(h_\\theta (x^{(i)}) - y^{(i)}\\Bigr)x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial Parameters \n",
      "\tTheta1 = 0 \n",
      "\tTheta0 = 0 \n",
      "\tCost: 32.073\n"
     ]
    }
   ],
   "source": [
    "m = y.size # number of training examples\n",
    "theta_gd=[[0],[0]]  # create first theta values as a list.. theta0 and theta1\n",
    "\n",
    "def computeCost(X, y, theta_gd):\n",
    "\n",
    "    J = 0\n",
    "    \n",
    "    hypoth = X.dot(theta_gd) # determin the hypothesis\n",
    "\n",
    "    J = 1 / (2 * m) * np.sum(np.square(hypoth - y)) # determine the cost function\n",
    "    \n",
    "    return(J) # return the cost function\n",
    "\n",
    "computeCost(X, y, theta_gd) # run the cos function\n",
    "\n",
    "J = computeCost(X, y, theta_gd) # run the cos function\n",
    "print('The initial Parameters \\n\\tTheta1 = 0 \\n\\tTheta0 = 0 \\n\\tCost: %.3f'%(J))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implemet Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The parameter values for minimised cost function:\n",
      "\tTheta1: -3.892863\n",
      "\tTheta2: 1.192740\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01 #define the learning rate \n",
    "num_iters = 2000 # define the number of iterations of gradient descent\n",
    "x_scale = np.arange(num_iters)\n",
    "\n",
    "def gradientDescent(X, y, theta_gd, alpha, num_iters):\n",
    "   \n",
    "    J_history = np.zeros(num_iters) # create array of zeros for the cost function values\n",
    "    theta_gd_history = np.zeros((num_iters,2)) # create array for the theta history values\n",
    "    \n",
    "    for iter in np.arange(num_iters): #gradient descent iteration loop\n",
    "        \n",
    "        hypoth = X.dot(theta_gd) # determine the hypothesis for the current value of thetas\n",
    "        dJ_theta_gd  = (1/m) * (X.T.dot(hypoth - y)) # Determine derivative of cost function\n",
    "        theta_gd = theta_gd - alpha * dJ_theta_gd #calculate new values of theta\n",
    "        \n",
    "        # store calculated values of theta  \n",
    "        theta_gd_history[iter][0] = theta_gd[0] \n",
    "        theta_gd_history[iter][1] = theta_gd[1]\n",
    "        \n",
    "        J_history[iter] = computeCost(X, y, theta_gd) #calclate the cost fucntion\n",
    "        \n",
    "    return(theta_gd, J_history, theta_gd_history) # return final theta and the list of cost function results\n",
    "\n",
    "# run gradient descent to return final theta for minimum cost funtiona as well as all the cost function values \n",
    "#calculated (for each iteration)\n",
    "theta_gd, J_history, theta_gd_history = gradientDescent(X, y, theta_gd, alpha, num_iters) \n",
    "print('\\nThe parameter values for minimised cost function:')\n",
    "print('\\tTheta1: %f' %(theta_gd[0]))  \n",
    "print('\\tTheta2: %f' %(theta_gd[1]))  \n",
    "\n",
    "# plot theta values as a function of iterations of gradient descent\n",
    "plt.figure()\n",
    "plt.plot(x_scale, theta_gd_history)\n",
    "plt.ylabel('Theta Values')\n",
    "plt.xlabel('Iterations');\n",
    "plt.title('Theta Values over iterations')\n",
    "plt.grid(True)\n",
    "plt.legend([r'$\\theta_0$', r'$\\theta_1$'])\n",
    "\n",
    "# plot cost function histtory as a function of iterations of gradient descent\n",
    "plt.figure()\n",
    "plt.plot(x_scale, J_history)\n",
    "plt.ylabel('Cost J')\n",
    "plt.xlabel('Iterations');\n",
    "plt.title('Cost Function over iterations')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Gradient Descent and sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodness of fit for Gradient Descent r^2: 0.702\n",
      "Goodness of fit for Sk.learn leanear model r^2: 0.702\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#detertine the y values according to the theta values from gradient descent\n",
    "y_gd = X.dot(theta_gd) \n",
    "#determine the r^2 value\n",
    "from sklearn.metrics import r2_score \n",
    "r2_gd = r2_score(y, y_gd) \n",
    "\n",
    "# determine the fit using linear regression classifier from sklearn\n",
    "from sklearn.linear_model import LinearRegression # data classification, regeression, clustering etc\n",
    "lr_regr = LinearRegression()\n",
    "lr_regr.fit(X[:,1].reshape(-1,1), y.ravel())\n",
    "theta_lr = np.zeros(2)\n",
    "theta_lr[0] = lr_regr.intercept_\n",
    "theta_lr[1] = lr_regr.coef_\n",
    "y_lr = X.dot(theta_lr)\n",
    "r2_lr = r2_score(y, y_lr) \n",
    "\n",
    "print('Goodness of fit for Gradient Descent r^2: %.3f'%(r2_gd))\n",
    "print('Goodness of fit for Sk.learn leanear model r^2: %.3f'%(r2_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "\t\t\tFITING RESULTS GRADIENT DESCENT\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Fitting: Gradient Descent\n",
      "Learning rate (alpha): 0.01\n",
      "Number of iterations: 2000\n",
      "Fitting parameters:\n",
      "\tIntercept θ_0: -3.788\n",
      "\tSlope θ_1: 1.182\n",
      "Goodness of fit r^2: 0.702\n",
      "-----------------------------------------------------------------\n",
      "\t\t\tFITING RESULTS SKLEARN\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Fitting: Linear Regression sklearn library\n",
      "Fitting parameters:\n",
      "\tIntercept θ_0: -3.896\n",
      "\tSlope θ_1: 1.193\n",
      "Goodness of fit r^2: 0.702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a1e921860>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('-----------------------------------------------------------------')\n",
    "print('\\t\\t\\tFITING RESULTS GRADIENT DESCENT')\n",
    "print('-----------------------------------------------------------------')\n",
    "print('\\nFitting: Gradient Descent')\n",
    "print('Learning rate (alpha): %.2f'%(alpha))\n",
    "print('Number of iterations: %.0f'%(num_iters))\n",
    "print('Fitting parameters:')\n",
    "print('\\tIntercept %s_0: %.3f'%(sympy.pretty(theta), theta_gd[0]))\n",
    "print('\\tSlope %s_1: %.3f'%(sympy.pretty(theta), theta_gd[1]))\n",
    "print('Goodness of fit r^2: %.3f'%(r2_gd))\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "print('\\t\\t\\tFITING RESULTS SKLEARN')\n",
    "print('-----------------------------------------------------------------')\n",
    "print('\\nFitting: Linear Regression sklearn library')\n",
    "print('Fitting parameters:')\n",
    "print('\\tIntercept %s_0: %.3f'%(sympy.pretty(theta), theta_lr[0]))\n",
    "print('\\tSlope %s_1: %.3f'%(sympy.pretty(theta), theta_lr[1]))\n",
    "print('Goodness of fit r^2: %.3f'%(r2_lr))\n",
    "\n",
    "# Plot gradient descent & Scikit-learn Linear regression \n",
    "plt.figure()\n",
    "plt.scatter(X[:,1], y, c = 'r', marker = 'x', linewidths = 2)\n",
    "plt.plot(X[:,1], y_gd, label = 'Linear regression (Gradient descent)') \n",
    "plt.plot(X[:,1],y_lr , label = 'Linear regression (Scikit-learn)')\n",
    "plt.xlim(4,24)\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profit Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict profit for a city with population of:\n",
      "Population of 35000 : 3497\n",
      "Population of 70000 : 44874\n"
     ]
    }
   ],
   "source": [
    "# Predct profit for a city with population of 35000 and 70000\n",
    "print('Predict profit for a city with population of:') \n",
    "print('Population of %.0f : %.0f' %(35000, theta_gd.T.dot([1, 3.5]) * 10000))\n",
    "print('Population of %.0f : %.0f' %(70000,theta_gd.T.dot([1, 7]) * 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-d Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create grid coordinates for plotting\n",
    "theta0_axis = np.linspace(-10, 10, 50) # create axis for theta0 grid\n",
    "theta1_axis = np.linspace(-1, 4, 50)# create axis for theta1 grid\n",
    "\n",
    "theta0_grid, theta1_grid = np.meshgrid(theta0_axis, theta1_axis, indexing='xy') # create mesh grid of theta0 and theta1\n",
    "Cost_3d = np.zeros((theta0_axis.size,theta1_axis.size)) # creats grid of zeros from cost values\n",
    "\n",
    "# Calculate Cost-values based on grid of coefficients\n",
    "for (i,j),v in np.ndenumerate(Cost_3d): #loop through the Cost grid ... double for loop\n",
    "    theta_3d = [[theta0_grid[i,j]], [theta1_grid[i,j]]] #create current value of theta\n",
    "    Cost_3d[i,j] = computeCost(X, y, theta_3d) #create cost for the give values of theta\n",
    "\n",
    "fig = plt.figure(figsize = (15,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "# Left plot\n",
    "CS = ax1.contour(theta0_grid, theta1_grid, Cost_3d, np.logspace(-2, 3, 20), cmap = plt.cm.jet)\n",
    "ax1.scatter(theta_gd[0],theta_gd[1], c = 'r') # plot the final theta values\n",
    "\n",
    "# Right plot\n",
    "ax2.plot_surface(theta0_grid, theta1_grid, Cost_3d, rstride = 1, cstride = 1, alpha = 0.6, cmap = plt.cm.jet)\n",
    "ax2.set_zlabel('Cost - J')\n",
    "ax2.set_zlim(Cost_3d.min(), Cost_3d.max())\n",
    "ax2.view_init(elev = 15, azim = 230)\n",
    "\n",
    "# settings common to both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r'$\\theta_0$', fontsize=17)\n",
    "    ax.set_ylabel(r'$\\theta_1$', fontsize=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
