{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load MATLAB files\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " \n",
    "#%config InlineBackend.figure_formats = {'pdf',}\n",
    "#%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.close(\"all\") # Clsoe all plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5000, 401) (with intercept)\n",
      "y: (5000, 1)\n",
      "\n",
      "Number of features (n): 401\n",
      "Number of training examples (nm): 5000\n",
      "\n",
      "theta1: (25, 401)\n",
      "theta2: (10, 26)\n",
      "\n",
      "Sample:\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = loadmat('ex3data1.mat')\n",
    "data.keys()\n",
    "\n",
    "#Load the weights\n",
    "weights = loadmat('ex3weights.mat')\n",
    "weights.keys()\n",
    "\n",
    "y = data['y']\n",
    "\n",
    "# Add x0 \n",
    "X = np.c_[np.ones((data['X'].shape[0],1)), data['X']]\n",
    "\n",
    "m,n = X.shape\n",
    " \n",
    "print('X: {} (with intercept)'.format(X.shape))\n",
    "print('y: {}'.format(y.shape))\n",
    "\n",
    "print('\\nNumber of features (n): %.0f'%(n))\n",
    "print('Number of training examples (nm): %.0f'%(m))\n",
    "\n",
    "theta1, theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "print('\\ntheta1: {}'.format(theta1.shape))\n",
    "print('theta2: {}'.format(theta2.shape))\n",
    "\n",
    "sample = np.random.choice(X.shape[0], 20)\n",
    "print('\\nSample:')\n",
    "plt.imshow(X[sample,1:].reshape(-1,20).T)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularisationc can reduce the weight of the parameters in anywa function by increaseing the respective cost.\n",
    "we edit the hypothesis to inflate the cost and thus reduce the fitting parameters\n",
    "\n",
    "Note: \\theta_0 is not to be regularised adn is calcualted seperately when using the vertorised method\n",
    "\n",
    "Regularized Cost Function\n",
    "for $\\theta = 1,2,3,...n$\n",
    "$$ J(\\theta) = -\\frac{1}{m}\\bigg[\\sum_{i=1}^{m}\\big[y^{(i)} log( h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))\\big] + \\frac{\\lambda}{2m}\\sum_{j = 1}^{n}\\theta_{j}^{2}\\bigg]$$\n",
    "\n",
    "\n",
    "Vectorized Cost Function\n",
    "$$ J(\\theta) = -\\frac{1}{m}\\big((log(h)^Ty+(log(1-h)^T(1-y)\\big) + \\frac{\\lambda}{2m}\\sum_{j = 1}^{n}\\theta_{j}^{2}$$\n",
    "\n",
    "Gradient Descent: Repeat until converge<br>\n",
    "for j = 0 (The intercept is not regularised)\n",
    "$$\\theta_0 : = \\theta_0- \\alpha\\frac{\\partial J(\\theta)}{\\partial\\theta_{0}}$$\n",
    "for j = 1,2,3...n\n",
    "$$\\theta_j : = \\theta_j- \\alpha\\Big[\\Big(\\frac{\\partial J(\\theta)}{\\partial\\theta_{j}}\\Big) + \\frac{\\lambda}{m}\\theta_{j}\\Big] $$\n",
    "\n",
    "\n",
    "Partial derivatives:<br>\n",
    "for j = 0 \n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial\\theta_{0}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{0} $$\n",
    "\n",
    "for j = 1,2,3...m \n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j} +\\frac{\\lambda}{m}\\theta_{j}^{2}$$\n",
    "\n",
    "Vectorized Partial Derivative <br>\n",
    "for j = 0 \n",
    "$$\\frac{\\partial J(\\theta)}{\\partial\\theta_{0}} = \\frac{1}{m} X^T(h-y)$$\n",
    "for j = 1,2,3... n\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m} X^T(h-y) + \\frac{\\lambda}{m}\\theta_{j}^{2}$$\n",
    "where $h = g(X\\theta)$\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularised Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return(1 / (1 + np.exp(-z)))\n",
    "    \n",
    "#logitic regression cost fucntion (Regularised)\n",
    "def lrcostFunctionReg(theta, reg, X, y):\n",
    "    m = y.size\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    \n",
    "    #cost fuinction\n",
    "    J = -1 * (1/m) * (np.log(h).T.dot(y) + np.log(1 - h).T.dot(1 - y)) + (reg/(2 * m))*np.sum(np.square(theta[1:]))\n",
    "    \n",
    "    if np.isnan(J[0]):\n",
    "        return(np.inf)\n",
    "    return(J[0])    \n",
    "\n",
    "# logistic regreession gradient of cost function\n",
    "def lrgradientReg(theta, reg, X,y):\n",
    "    m = y.size\n",
    "    h = sigmoid(X.dot(theta.reshape(-1,1)))\n",
    "     \n",
    "    #Gradient\n",
    "    grad = (1/m)*X.T.dot(h-y) + (reg/m)*np.r_[[[0]],theta[1:].reshape(-1,1)]\n",
    "        \n",
    "    return(grad.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One - Vrs - All Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.39142338e+00   0.00000000e+00   0.00000000e+00 ...,   6.90403614e-04\n",
      "    1.25548916e-07   0.00000000e+00]\n",
      " [ -3.01137869e+00   0.00000000e+00   0.00000000e+00 ...,   1.48908371e-03\n",
      "   -1.69495224e-04   0.00000000e+00]\n",
      " [ -4.51398334e+00   0.00000000e+00   0.00000000e+00 ...,  -1.60669585e-05\n",
      "    3.43368479e-07   0.00000000e+00]\n",
      " ..., \n",
      " [ -8.41428002e+00   0.00000000e+00   0.00000000e+00 ...,  -6.96539602e-05\n",
      "    6.26414118e-06   0.00000000e+00]\n",
      " [ -5.13253825e+00   0.00000000e+00   0.00000000e+00 ...,  -1.54282889e-04\n",
      "    5.92217956e-06   0.00000000e+00]\n",
      " [ -4.38685169e+00   0.00000000e+00   0.00000000e+00 ...,  -3.60354278e-04\n",
      "    9.53842298e-06   0.00000000e+00]]\n",
      "Training set accuracy with Obe-Vrs_all: 93.24 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# logistic regression - one - Vrs - all\n",
    "def oneVsAll(features, classes, n_labels, reg):\n",
    "    initial_theta = np.zeros((X.shape[1],1))  # 401x1\n",
    "    all_theta = np.zeros((n_labels, X.shape[1])) #10x401\n",
    "\n",
    "    for c in np.arange(1, n_labels + 1):\n",
    "        res = minimize(lrcostFunctionReg, initial_theta, args = (reg, features, (classes == c) * 1), \n",
    "                       method = None,jac = lrgradientReg, options = {'maxiter':50})\n",
    "        all_theta[c-1] = res.x\n",
    "    return(all_theta)\n",
    "\n",
    "\n",
    "theta = oneVsAll(X, y, 10, 0.1)\n",
    "print(theta)\n",
    "\n",
    "\n",
    "def predictOneVsAll(all_theta, features):\n",
    "    probs = sigmoid(X.dot(all_theta.T))\n",
    "        \n",
    "    # Adding one because Python uses zero based indexing for the 10 columns (0-9),\n",
    "    # while the 10 classes are numbered from 1 to 10.\n",
    "    return(np.argmax(probs, axis = 1) + 1)\n",
    " \n",
    "pred = predictOneVsAll(theta, X)\n",
    "\n",
    "print('Training set accuracy with Obe-Vrs_all: {} %'.format(np.mean(pred == y.ravel())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Logistic Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy with sklearn: 96.5 %\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C = 10, penalty='l2', solver='liblinear')\n",
    "# Scikit-learn fits intercept automatically, so we exclude first column with 'ones' from X when fitting.\n",
    "clf.fit(X[:,1:],y.ravel())\n",
    "\n",
    "pred2 = clf.predict(X[:,1:])\n",
    "print('Training set accuracy with sklearn: {} %'.format(np.mean(pred2 == y.ravel())*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 97.52 %\n"
     ]
    }
   ],
   "source": [
    "def predict(theta_1, theta_2, features):\n",
    "    z2 = theta_1.dot(features.T)\n",
    "    a2 = np.c_[np.ones((data['X'].shape[0],1)), sigmoid(z2).T]\n",
    "    \n",
    "    z3 = a2.dot(theta_2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "        \n",
    "    return(np.argmax(a3, axis=1)+1) \n",
    "\n",
    "pred = predict(theta1, theta2, X)\n",
    "print('Training set accuracy: {} %'.format(np.mean(pred == y.ravel())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
